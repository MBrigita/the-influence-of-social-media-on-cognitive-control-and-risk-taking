{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined = pd.read_excel(\"../Downloads/updated_podatki.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     IDkoda  SEX  AGE  Q2  Q3  Q4  Q5  Q9_2   Q6   Q8  ...  Snapchat actual  \\\n",
      "0   matt987    2   36   2   2   2   2   NaN  2.0  NaN  ...                0   \n",
      "1   adef331    1   22   2   2   1   1   2.0  NaN  2.0  ...               15   \n",
      "2   xdja222    1   23   1   2   2   1   2.0  NaN  2.0  ...               62   \n",
      "4   135asdf    2   20   2   2   1   1   NaN  2.0  NaN  ...                0   \n",
      "5   dobe123    1   25   1   2   1   1   NaN  2.0  NaN  ...                0   \n",
      "..      ...  ...  ...  ..  ..  ..  ..   ...  ...  ...  ...              ...   \n",
      "93  toto237    2   21   2   2   1   1   NaN  1.0  NaN  ...               16   \n",
      "94  rike385    1   21   1   1   1   1   NaN  1.0  NaN  ...                3   \n",
      "95  javs374    2   26   1   2   2   1   NaN  1.0  NaN  ...                0   \n",
      "96  mark247    1   21   1   2   1   1   1.0  NaN  1.0  ...                9   \n",
      "97  987ejej    2   21   2   2   2   2   2.0  NaN  1.0  ...               30   \n",
      "\n",
      "    Threads actual  Pinterest actual  other actual percenterrors      meanRT  \\\n",
      "0                0                 0             0      0.000000  597.383333   \n",
      "1                0                 0            85      0.000000  566.033333   \n",
      "2                0                 0             0      0.000000  486.433333   \n",
      "4                0                 0            10      0.000000  427.533333   \n",
      "5                0                 0             0      1.666667  386.762712   \n",
      "..             ...               ...           ...           ...         ...   \n",
      "93               0                 3            24      0.000000  547.716667   \n",
      "94               0                 0            75      0.000000  662.566667   \n",
      "95               0                 0           273      3.333333  527.155172   \n",
      "96               0                 0            70      0.000000  519.500000   \n",
      "97               0                 0             0      3.333333  808.051724   \n",
      "\n",
      "    percenterrors_c    meanrt_c  percenterrors_ic   meanrt_ic  \n",
      "0          0.000000  572.033333          0.000000  622.733333  \n",
      "1          0.000000  543.000000          0.000000  589.066667  \n",
      "2          0.000000  508.366667          0.000000  464.500000  \n",
      "4          0.000000  391.966667          0.000000  463.100000  \n",
      "5          0.000000  386.933333          3.333333  386.586207  \n",
      "..              ...         ...               ...         ...  \n",
      "93         0.000000  501.833333          0.000000  593.600000  \n",
      "94         0.000000  646.800000          0.000000  678.333333  \n",
      "95         3.333333  517.724138          3.333333  536.586207  \n",
      "96         0.000000  466.066667          0.000000  572.933333  \n",
      "97         0.000000  769.166667          6.666667  849.714286  \n",
      "\n",
      "[93 rows x 44 columns]\n"
     ]
    }
   ],
   "source": [
    "# Selecting columns you want to check for outliers\n",
    "columns_to_check = ['percenterrors', 'meanRT','percenterrors_c', 'meanrt_c', 'percenterrors_ic', 'meanrt_ic']  # Replace with actual column names\n",
    "# Calculate Z-score for the selected columns\n",
    "z_scores = np.abs(stats.zscore(df_joined[columns_to_check]))\n",
    "\n",
    "# Define a threshold for outliers (e.g., z-score > 3)\n",
    "threshold = 3\n",
    "\n",
    "# Create a mask to filter out rows with outliers in any of the selected columns\n",
    "outlier_mask = (z_scores > threshold).any(axis=1)\n",
    "\n",
    "# Filter out rows with outliers\n",
    "df_cleaned = df_joined[~outlier_mask]\n",
    "print(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_check_time = ['Perception of time on all social media','Instagram perception','TikTok perception', 'FB perception',\n",
    "       'whatsApp perception', 'LinkedIn perception', 'Twitter perception',\n",
    "       'Youtube perception', 'Snapchat perception', 'Threads perception',\n",
    "       'Pinterest perception', 'other perception',\n",
    "       'time spend on all social media apps actual', 'Instagram actual',\n",
    "       'TikTok actual', 'FB actual', 'whatsApp actual', 'LinkedIn actual',\n",
    "       'Twitter actual', 'Youtube actual', 'Snapchat actual', 'Threads actual',\n",
    "       'Pinterest actual', 'other actual']\n",
    "\n",
    "# Calculate Z-score for the selected columns\n",
    "z_scores_time = np.abs(stats.zscore(df_cleaned[columns_to_check_time]))\n",
    "\n",
    "# Define a threshold for outliers (e.g., z-score > 3)\n",
    "threshold = 3\n",
    "\n",
    "# Create a mask to filter out rows with outliers in any of the selected columns\n",
    "outlier_mask_time = (z_scores_time > threshold).any(axis=1)\n",
    "\n",
    "# Filter out rows with outliers\n",
    "df_cleaned_time = df_cleaned[~outlier_mask_time]\n",
    "df_cleaned_time = df_cleaned_time.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['IDkoda', 'SEX', 'AGE', 'Q2', 'Q3', 'Q4', 'Q5', 'Q9_2', 'Q6', 'Q8',\n",
      "       'Q9', 'Q7a', 'Q7b', 'Perception of time on all social media',\n",
      "       'Other social media', 'Instagram perception', 'TikTok perception',\n",
      "       'FB perception', 'whatsApp perception', 'LinkedIn perception',\n",
      "       'Twitter perception', 'Youtube perception', 'Snapchat perception',\n",
      "       'Threads perception', 'Pinterest perception', 'other perception',\n",
      "       'time spend on all social media apps actual', 'Instagram actual',\n",
      "       'TikTok actual', 'FB actual', 'whatsApp actual', 'LinkedIn actual',\n",
      "       'Twitter actual', 'Youtube actual', 'Snapchat actual', 'Threads actual',\n",
      "       'Pinterest actual', 'other actual', 'percenterrors', 'meanRT',\n",
      "       'percenterrors_c', 'meanrt_c', 'percenterrors_ic', 'meanrt_ic'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_cleaned_time.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             SEX        AGE         Q2         Q3         Q4         Q5  \\\n",
      "count  77.000000  77.000000  77.000000  77.000000  77.000000  77.000000   \n",
      "mean    1.623377  22.649351   1.805195   1.870130   1.168831   1.285714   \n",
      "std     0.538980   4.803415   0.398648   0.338365   0.377059   0.454716   \n",
      "min     1.000000  20.000000   1.000000   1.000000   1.000000   1.000000   \n",
      "25%     1.000000  21.000000   2.000000   2.000000   1.000000   1.000000   \n",
      "50%     2.000000  22.000000   2.000000   2.000000   1.000000   1.000000   \n",
      "75%     2.000000  23.000000   2.000000   2.000000   1.000000   2.000000   \n",
      "max     3.000000  59.000000   2.000000   2.000000   2.000000   2.000000   \n",
      "\n",
      "            Q9_2         Q6         Q8         Q9  ...  Snapchat actual  \\\n",
      "count  36.000000  41.000000  36.000000  41.000000  ...        77.000000   \n",
      "mean    1.694444   1.292683   1.277778   1.780488  ...        25.363636   \n",
      "std     0.467177   0.460646   0.454257   0.419058  ...        46.383955   \n",
      "min     1.000000   1.000000   1.000000   1.000000  ...         0.000000   \n",
      "25%     1.000000   1.000000   1.000000   2.000000  ...         0.000000   \n",
      "50%     2.000000   1.000000   1.000000   2.000000  ...         3.000000   \n",
      "75%     2.000000   2.000000   2.000000   2.000000  ...        26.000000   \n",
      "max     2.000000   2.000000   2.000000   2.000000  ...       223.000000   \n",
      "\n",
      "       Threads actual  Pinterest actual  other actual  percenterrors  \\\n",
      "count       77.000000         77.000000     77.000000      77.000000   \n",
      "mean         0.233766          2.025974     21.428571       1.709957   \n",
      "std          1.265910          5.645152     45.930849       3.010387   \n",
      "min          0.000000          0.000000      0.000000       0.000000   \n",
      "25%          0.000000          0.000000      0.000000       0.000000   \n",
      "50%          0.000000          0.000000      0.000000       0.000000   \n",
      "75%          0.000000          0.000000     18.000000       1.666667   \n",
      "max         10.000000         30.000000    210.000000      18.333333   \n",
      "\n",
      "           meanRT  percenterrors_c    meanrt_c  percenterrors_ic   meanrt_ic  \n",
      "count   77.000000        77.000000   77.000000         77.000000   77.000000  \n",
      "mean   539.630771         0.822511  512.437887          2.597403  567.395701  \n",
      "std     92.354336         2.304067   89.456926          4.140210  100.580000  \n",
      "min    386.762712         0.000000  365.793103          0.000000  386.586207  \n",
      "25%    476.416667         0.000000  458.033333          0.000000  495.965517  \n",
      "50%    535.016667         0.000000  501.833333          0.000000  557.633333  \n",
      "75%    567.627119         0.000000  551.200000          3.333333  617.571429  \n",
      "max    839.593220        13.333333  776.633333         23.333333  901.266667  \n",
      "\n",
      "[8 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "# Display the summary of the data\n",
    "print(df_cleaned_time.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average perceived and actual time spent on social media\n",
    "perceived_cols = [col for col in df_cleaned_time.columns if \"perception\" in col]\n",
    "actual_cols = [col for col in df_cleaned_time.columns if \"actual\" in col]\n",
    "\n",
    "\n",
    "df_cleaned_time['difference'] = df_cleaned_time['Perception of time on all social media'] - df_cleaned_time['time spend on all social media apps actual']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     77.000000\n",
      "mean    -263.636364\n",
      "std      121.159194\n",
      "min     -583.000000\n",
      "25%     -326.000000\n",
      "50%     -256.000000\n",
      "75%     -185.000000\n",
      "max       -4.000000\n",
      "Name: difference, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df_cleaned_time['difference'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jh/x3pzkf253nz6__d6tfg4p4200000gn/T/ipykernel_58712/3488168606.py:8: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  risk_taking_data_binary = risk_taking_data.applymap(lambda x: 1 if x == 1 else (0 if x == 2 else np.nan))\n"
     ]
    }
   ],
   "source": [
    "# Analyze risk-taking behavior\n",
    "lotteries_columns = df_cleaned_time.columns[3:13]\n",
    "lotteries_data = df_cleaned_time[lotteries_columns]\n",
    "\n",
    "# 2 represents safe option, 1 represents risky option\n",
    "risk_taking_data = lotteries_data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Binarize the risk-taking behavior for logistic regression (0 = safe, 1 = risky)- riksy has value 1\n",
    "risk_taking_data_binary = risk_taking_data.applymap(lambda x: 1 if x == 1 else (0 if x == 2 else np.nan))\n",
    "#beacuse in q6 and q9_2 the risky option is the option 1 (others is option 2)\n",
    "risk_taking_data_binary['Q6']=risk_taking_data_binary['Q6'].apply(lambda x: 1 if x == 0 else (0 if x == 1 else np.nan))\n",
    "risk_taking_data_binary['Q9_2']=risk_taking_data_binary['Q9_2'].apply(lambda x: 1 if x ==0 else (0 if x == 1 else np.nan))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('Q9_2', 'Q6'): (3.793360469954363, 0.00029886039852298075),\n",
       " ('Q8', 'Q9'): (5.049986187015259, 3.0107527558838038e-06),\n",
       " ('Q7a', 'Q7b'): (-5.072570263045555, 2.7564691806498778e-06)}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Define lottery pairs for gain and loss domains\n",
    "lottery_pairs = {\n",
    "    'Q9_2': 'Q6',\n",
    "    'Q8': 'Q9',\n",
    "    'Q7a': 'Q7b'\n",
    "}\n",
    "\n",
    "# Prepare data for t-tests and conduct the tests\n",
    "t_test_results = {}\n",
    "for loss_col, gain_col in lottery_pairs.items():\n",
    "    loss_data = risk_taking_data_binary[loss_col].dropna()\n",
    "    gain_data = risk_taking_data_binary[gain_col].dropna()\n",
    "    \n",
    "    t_stat, p_value = stats.ttest_ind(loss_data, gain_data)\n",
    "    \n",
    "    t_test_results[(loss_col, gain_col)] = (t_stat, p_value)\n",
    "\n",
    "t_test_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair: ('Q9_2', 'Q6'), Result: {'chi2': 10.837915316230863, 'p-value': 0.00099442626638407, 'degrees_of_freedom': 1, 'expected_frequencies': array([[18.7012987, 17.2987013],\n",
      "       [21.2987013, 19.7012987]])}\n",
      "Pair: ('Q8', 'Q9'), Result: {'chi2': 17.563668699186998, 'p-value': 2.7784632549262468e-05, 'degrees_of_freedom': 1, 'expected_frequencies': array([[19.63636364, 16.36363636],\n",
      "       [22.36363636, 18.63636364]])}\n",
      "Pair: ('Q7b', 'Q7a'), Result: {'chi2': 17.693850824910275, 'p-value': 2.5946401470491066e-05, 'degrees_of_freedom': 1, 'expected_frequencies': array([[19.7012987, 21.2987013],\n",
      "       [17.2987013, 18.7012987]])}\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "# Define the pairs and their respective domains\n",
    "pairs = {\n",
    "    ('Q9_2', 'Q6'): ('loss', 'gain'),\n",
    "    ('Q8', 'Q9'): ('loss', 'gain'),\n",
    "    ('Q7b', 'Q7a'): ('loss', 'gain')\n",
    "}\n",
    "\n",
    "\n",
    "results = {}\n",
    "# Function to perform chi-squared test and return the result\n",
    "def chi_squared_test(counts1, counts2):\n",
    "    # Create contingency table\n",
    "    contingency_table = pd.DataFrame([counts1, counts2], columns=['safe', 'risky'])\n",
    "    # Perform chi-squared test\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "    return chi2, p, dof, expected\n",
    "\n",
    "# Separate and aggregate responses for each pair\n",
    "for (col1, col2), (domain1, domain2) in pairs.items():\n",
    "    df_loss = risk_taking_data_binary[col1].dropna().value_counts().sort_index()\n",
    "    df_gain = risk_taking_data_binary[col2].dropna().value_counts().sort_index()\n",
    "    \n",
    "    # Ensure both counts have safe and risky categories\n",
    "    df_loss = df_loss.reindex([0, 1], fill_value=0)\n",
    "    df_gain = df_gain.reindex([0, 1], fill_value=0)\n",
    "    \n",
    "    counts_loss = df_loss.values\n",
    "    counts_gain = df_gain.values\n",
    "    \n",
    "    if counts_loss.sum() > 0 and counts_gain.sum() > 0:\n",
    "        chi2, p, dof, expected = chi_squared_test(counts_loss, counts_gain)\n",
    "        results[(col1, col2)] = {\n",
    "            'chi2': chi2,\n",
    "            'p-value': p,\n",
    "            'degrees_of_freedom': dof,\n",
    "            'expected_frequencies': expected\n",
    "        }\n",
    "    else:\n",
    "        results[(col1, col2)] = 'Not enough data for chi-squared test'\n",
    "\n",
    "# Print results\n",
    "for pair, result in results.items():\n",
    "    print(f\"Pair: {pair}, Result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jh/x3pzkf253nz6__d6tfg4p4200000gn/T/ipykernel_58712/2529344321.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sm_data['Perception of time on all social media']=sm_data['Perception of time on all social media']*60\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0      60\n",
       "1     360\n",
       "2     240\n",
       "3     240\n",
       "4     240\n",
       "     ... \n",
       "72    240\n",
       "73    240\n",
       "74    120\n",
       "75    240\n",
       "76    360\n",
       "Name: Perception of time on all social media, Length: 77, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyze social media usage\n",
    "\n",
    "sm_columns = ['Perception of time on all social media',\n",
    "       'Other social media', 'Instagram perception', 'TikTok perception',\n",
    "       'FB perception', 'whatsApp perception', 'LinkedIn perception',\n",
    "       'Twitter perception', 'Youtube perception', 'Snapchat perception',\n",
    "       'Threads perception', 'Pinterest perception', 'other perception',\n",
    "       'time spend on all social media apps actual', 'Instagram actual',\n",
    "       'TikTok actual', 'FB actual', 'whatsApp actual', 'LinkedIn actual',\n",
    "       'Twitter actual', 'Youtube actual', 'Snapchat actual', 'Threads actual',\n",
    "       'Pinterest actual', 'other actual']\n",
    "sm_data = df_cleaned_time[sm_columns]\n",
    "\n",
    "sm_data['Perception of time on all social media']=sm_data['Perception of time on all social media']*60\n",
    "sm_data['Perception of time on all social media']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
